{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнаружение людей на видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала подготовим необходимые функции.  \n",
    "Импортируем openCV и напишем функцию, с помощью которой можно обнаруживать людей на видео и сохранять результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_video_with_people(input_path, output_path ,model, get_bboxes_func, desired_class):\n",
    "\n",
    "    # Инициализация \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Проверка, успешно ли открыто видео\n",
    "    if not cap.isOpened():\n",
    "        print(\"Не удалось открыть видео\")\n",
    "        exit()\n",
    "        \n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    # Создание объекта VideoWriter для сохранения результата\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Кодек для MP4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        # Захват кадра \n",
    "        ret, frame = cap.read()\n",
    "     \n",
    "        boxes, classes, confidences = get_bboxes_func(model, frame)\n",
    "        \n",
    "        for box, cls, conf in zip(boxes, classes, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)  # Преобразование координат в целые числа\n",
    "            \n",
    "            # Проверка, что объект - человек\n",
    "            if cls == desired_class:\n",
    "                # Отрисовка bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Добавление текста с классом и уверенностью\n",
    "                label = f\"Person {conf:.2f}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        # Ресайз\n",
    "        scale = 0.5\n",
    "        frame_resized = cv2.resize(frame, (-1, -1), fx=scale, fy=scale)\n",
    "\n",
    "        # Отображение кадра\n",
    "        cv2.imshow('video', frame_resized)\n",
    "\n",
    "        # Запись кадра в выходной видеофайл\n",
    "        out.write(frame)\n",
    "\n",
    "        # Выход из цикла по нажатию клавиши 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Освобождение ресурсов\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print( f\"Видео обработано и сохранено в файл: {output_path}\"  )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения bounding boxes моделей yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_yolo(model, img, imgsz = 640, conf = 0.4, iou = 0.7):\n",
    "    results = model(img,\n",
    "    imgsz = imgsz,\n",
    "    conf = conf,\n",
    "    iou = iou,\n",
    "    verbose = False\n",
    "    )\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        classes = result.boxes.cls.cpu().numpy()\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "    return boxes, classes, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_PATH = 'crowd.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве моделей для задачи детекции возьмем модели YOLOv11 nano - самую легковесную, а также YOLOv11 extra large - самую тяжелую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  'source' is missing. Using 'source=C:\\Users\\79252\\Desktop\\\\   2025\\env\\Lib\\site-packages\\ultralytics\\assets'.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_yolo_nano \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mget_video_with_people\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/output_yolo_nano.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_yolo_nano\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_bboxes_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_bboxes_yolo\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mget_video_with_people\u001b[1;34m(input_path, output_path, model, get_bboxes_func, desired_class)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Ресайз\u001b[39;00m\n\u001b[0;32m     44\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m---> 45\u001b[0m frame_resized \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Отображение кадра\u001b[39;00m\n\u001b[0;32m     48\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m, frame_resized)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_yolo_nano = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "get_video_with_people(input_path=FILE_PATH, output_path='results/output_yolo_nano.mp4', \n",
    "                      model = model_yolo_nano, get_bboxes_func = get_bboxes_yolo , desired_class = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала прогоним YOLOv11 nano. Модель достаточно быстро отработала, но на видео распознала далеко не всех людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yolo_extra_large = YOLO(\"yolo11x.pt\")\n",
    "get_video_with_people(input_path=FILE_PATH, output_path='results/output_yolo_extra_large.mp4', \n",
    "                      model = model_yolo_extra_large, get_bboxes_func = get_bboxes_yolo , desired_class = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем YOLOv11 extra large. Инференс намного дольше, но и качество также сильно выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем DETR из библиотеки transformers. Эта модель имеет в себе как и сверточные слои, так и подстроенную под задачу детекции часть архитектуры трансформер.  \n",
    "На ней качество должно быть намного лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_processor_for_detr = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\",use_fast=True)\n",
    "model_detr = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_detr( model_detr,  img, threshold=0.4):\n",
    "    \n",
    "    inputs = image_processor_for_detr(images=img, return_tensors=\"pt\")\n",
    "    outputs = model_detr(**inputs)\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    target_sizes = torch.tensor([[height, width]])\n",
    "    results = image_processor_for_detr.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    confidences , classes, boxes  = results[\"scores\"].cpu().detach().numpy(), results[\"labels\"].cpu().detach().numpy(), results[\"boxes\"].cpu().detach().numpy()\n",
    "    return boxes, classes, confidences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео обработано и сохранено в файл: output_detr.mp4\n"
     ]
    }
   ],
   "source": [
    "get_video_with_people(input_path=FILE_PATH, output_path='results/output_detr.mp4', \n",
    "                      model = model_detr, get_bboxes_func = get_bboxes_detr , desired_class = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, качество детекции у DETR намного лучше, чем у YOLOv11. При этом инференс был самый долгий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: было попробовано несколько архитектур: YOLOv11 с разным количеством весов и DETR - нейросеть для детекции с attention. Так как задачи сделать распознавание людей в реальном времени не стояло, можно смело брать модель с лучшим качеством распознавания, несмотря на время инференса. В нашем случае это DETR. Если же необходимо выполнять real time детекцию людей, то YOLO, которая была создана именно для этой задачи, прекрасно для этого подойдет.  \n",
    "\n",
    "Как можно улучшить качество распознавания? Если не требуется детектить людей в реальном времени, то можно смело брать трансформерную архитектуру типа DETR и его аналогов, и делать расчеты удаленно.  \n",
    "Если же критично время распознавания, можно брать SSD или YOLO. Улучшать качество возможно, например, с помощью таких вещей, как разбиение изображения на патчи для детекции большего числа объектов ([Ссылка](https://github.com/Koldim2001/YOLO-Patch-Based-Inference)), или другими методами.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
