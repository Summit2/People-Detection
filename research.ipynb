{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнаружение людей на видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала подготовим необходимые функции.  \n",
    "Импортируем openCV и напишем функцию, с помощью которой можно обнаруживать людей на видео и сохранять результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_video_with_people(input_path, output_path ,model, get_bboxes_func, desired_class):\n",
    "\n",
    "    # Инициализация \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Проверка, успешно ли открыто видео\n",
    "    if not cap.isOpened():\n",
    "        print(\"Не удалось открыть видео\")\n",
    "        exit()\n",
    "        \n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    # Создание объекта VideoWriter для сохранения результата\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Кодек для MP4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        # Захват кадра \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        boxes, classes, confidences = get_bboxes_func(model, frame)\n",
    "        \n",
    "        for box, cls, conf in zip(boxes, classes, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)  # Преобразование координат в целые числа\n",
    "            \n",
    "            # Проверка, что объект - человек\n",
    "            if cls == desired_class:\n",
    "                # Отрисовка bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Добавление текста с классом и уверенностью\n",
    "                label = f\"Person {conf:.2f}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Ресайз\n",
    "        scale = 0.5\n",
    "        frame_resized = cv2.resize(frame, (-1, -1), fx=scale, fy=scale)\n",
    "\n",
    "        # Отображение кадра\n",
    "        cv2.imshow('video', frame_resized)\n",
    "\n",
    "        # Запись кадра в выходной видеофайл\n",
    "        out.write(frame)\n",
    "\n",
    "        # Выход из цикла по нажатию клавиши 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Освобождение ресурсов\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print( f\"Видео обработано и сохранено в файл: {output_path}\"  )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения bounding boxes моделей yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_yolo(model, img, imgsz = 640, conf = 0.4, iou = 0.7):\n",
    "    results = model(img,\n",
    "    imgsz = imgsz,\n",
    "    conf = conf,\n",
    "    iou = iou,\n",
    "    verbose = False\n",
    "    )\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        classes = result.boxes.cls.cpu().numpy()\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "    return boxes, classes, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_PATH = 'crowd.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве моделей для задачи детекции возьмем модели YOLOv11 nano - самую легковесную, а также YOLOv11 extra large - самую тяжелую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео обработано и сохранено в файл: results/output_yolo_nano.mp4\n"
     ]
    }
   ],
   "source": [
    "model_yolo_nano = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "get_video_with_people(input_path=FILE_PATH, output_path='results/output_yolo_nano.mp4', \n",
    "                      model = model_yolo_nano, get_bboxes_func = get_bboxes_yolo , desired_class = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала прогоним YOLOv11 nano. Модель достаточно быстро отработала, но на видео распознала далеко не всех людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt to 'yolo11x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109M/109M [00:06<00:00, 19.0MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео обработано и сохранено в файл: results/output_yolo_extra_large.mp4\n"
     ]
    }
   ],
   "source": [
    "model_yolo_extra_large = YOLO(\"yolo11x.pt\")\n",
    "get_video_with_people(input_path=FILE_PATH, output_path='results/output_yolo_extra_large.mp4', \n",
    "                      model = model_yolo_extra_large, get_bboxes_func = get_bboxes_yolo , desired_class = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем YOLOv11 extra large. Инференс намного дольше, но и качество также сильно выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем DETR из библиотеки transformers. Эта модель имеет в себе как и сверточные слои, так и подстроенную под задачу детекции часть архитектуры трансформер.  \n",
    "На ней качество должно быть намного лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\79252\\Desktop\\Программирование\\Тестовое задание Консалтинг 2025\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_processor_for_detr = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\",use_fast=True)\n",
    "model_detr = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes_detr( model_detr,  img, threshold=0.4):\n",
    "    \n",
    "    inputs = image_processor_for_detr(images=img, return_tensors=\"pt\")\n",
    "    outputs = model_detr(**inputs)\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    target_sizes = torch.tensor([[height, width]])\n",
    "    results = image_processor_for_detr.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    confidences , classes, boxes  = results[\"scores\"].cpu().detach().numpy(), results[\"labels\"].cpu().detach().numpy(), results[\"boxes\"].cpu().detach().numpy()\n",
    "    return boxes, classes, confidences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео обработано и сохранено в файл: results/output_detr.mp4\n"
     ]
    }
   ],
   "source": [
    "get_video_with_people(input_path=FILE_PATH, output_path='results/output_detr.mp4', \n",
    "                      model = model_detr, get_bboxes_func = get_bboxes_detr , desired_class = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, качество детекции у DETR намного лучше, чем у YOLOv11. При этом инференс был самый долгий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: было попробовано несколько архитектур: YOLOv11 с разным количеством весов и DETR - нейросеть для детекции с attention. Так как задачи сделать распознавание людей в реальном времени не стояло, можно смело брать модель с лучшим качеством распознавания, несмотря на время инференса. В нашем случае это DETR. Если же необходимо выполнять real time детекцию людей, то YOLO, которая была создана именно для этой задачи, прекрасно для этого подойдет.  \n",
    "\n",
    "Как можно улучшить качество распознавания? Если не требуется детектить людей в реальном времени, то можно смело брать трансформерную архитектуру типа DETR и его аналогов, и делать расчеты удаленно.  \n",
    "Если же критично время распознавания, можно брать SSD или YOLO. Улучшать качество возможно, например, с помощью таких вещей, как разбиение изображения на патчи для детекции большего числа объектов ([Ссылка](https://github.com/Koldim2001/YOLO-Patch-Based-Inference)), или другими методами.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
